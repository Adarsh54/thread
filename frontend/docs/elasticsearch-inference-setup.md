# How to create an Elasticsearch inference endpoint (no OpenAI key)

You need an **inference endpoint** in your Elasticsearch cluster so the app can turn text (search query, product descriptions) into vectors. No separate “inference API key” — you use the same **ELASTICSEARCH_URL** and **ELASTICSEARCH_API_KEY** you already have.

---

## What is Kibana?

**Kibana** is the web UI for your Elasticsearch cluster. You use it to manage the cluster, run queries, and set up things like inference. It’s included with Elastic Cloud.

---

## Step 1: Open Kibana

1. Go to **[cloud.elastic.co](https://cloud.elastic.co)** and log in.
2. Open your **deployment** (the one whose URL you use for `ELASTICSEARCH_URL`).
3. In the deployment view, under **Applications**, click **Launch** (or **Open**) next to **Kibana**.  
   Kibana opens in a new tab.

---

## Step 2: Find or create an inference endpoint

### Option A: Use the default E5 endpoint (easiest)

Some Elastic Cloud deployments already have a built-in text-embedding endpoint:

- **Endpoint ID:** `.multilingual-e5-small-elasticsearch`
- **Model:** E5 small (384-dimensional vectors)

**Try it:**

1. In Kibana, open the main menu (☰) and go to **Machine Learning** → **Inference endpoints** (or **Inference**).
2. Check the list for an endpoint named like **.multilingual-e5-small-elasticsearch** or **E5**.
3. If you see it, note the **exact ID** (e.g. `.multilingual-e5-small-elasticsearch`).

**In your app:**

1. In **`.env.local`** set:
   ```bash
   ELASTICSEARCH_INFERENCE_ID=.multilingual-e5-small-elasticsearch
   ELASTICSEARCH_EMBEDDING_DIMENSION=384
   ```
2. Restart the app. The first search (or **POST /api/search/index**) will create the index with 384-dim vectors and use this endpoint for embeddings.

---

### Option B: Create an endpoint in Kibana

1. In Kibana: **Machine Learning** → **Inference endpoints** (or **Inference**).
2. Click **Create inference endpoint** (or **Add inference endpoint**).
3. Choose **Task type**: **Text embedding**.
4. Choose **Service**:
   - **Elasticsearch** – use a built-in model (e.g. E5). Pick the model (e.g. `.multilingual-e5-small`). The endpoint ID will be something you choose or one Elastic suggests.
   - **OpenAI** / **Cohere** / etc. – use an external API (you’d need that provider’s API key).
5. Give it an **Inference endpoint ID** (e.g. `my-embedding`) and save.
6. In **`.env.local`** set:
   ```bash
   ELASTICSEARCH_INFERENCE_ID=my-embedding
   ```
   If the model is E5 small (384 dims), also set:
   ```bash
   ELASTICSEARCH_EMBEDDING_DIMENSION=384
   ```

---

### Option C: Create an endpoint via API (curl)

If you prefer the API (same credentials as your app):

**E5 small (384 dimensions):**

```bash
curl -X PUT "YOUR_ELASTICSEARCH_URL/_inference/text_embedding/e5-small" \
  -H "Content-Type: application/json" \
  -H "Authorization: ApiKey YOUR_ELASTICSEARCH_API_KEY" \
  -d '{
    "service": "elasticsearch",
    "service_settings": {
      "model_id": ".multilingual-e5-small"
    }
  }'
```

Then in **`.env.local`**:

```bash
ELASTICSEARCH_INFERENCE_ID=e5-small
ELASTICSEARCH_EMBEDDING_DIMENSION=384
```

Replace `YOUR_ELASTICSEARCH_URL` and `YOUR_ELASTICSEARCH_API_KEY` with your real values.

---

## Step 3: Sync products and search

1. Make sure products exist in Supabase (with or without an `embedding` column).
2. Call **POST** `http://localhost:3000/api/search/index` (with the app running), or run a search so the app auto-syncs when the index is empty.
3. Use the search UI; embeddings are generated by Elasticsearch inference.

---

## Summary

| You want | Do this |
|----------|--------|
| Use default E5 | Set `ELASTICSEARCH_INFERENCE_ID=.multilingual-e5-small-elasticsearch` and `ELASTICSEARCH_EMBEDDING_DIMENSION=384`. |
| Create endpoint in Kibana | Machine Learning → Inference endpoints → Create; then set `ELASTICSEARCH_INFERENCE_ID` (and `ELASTICSEARCH_EMBEDDING_DIMENSION=384` for E5 small). |
| Create via API | `PUT _inference/text_embedding/{id}` as above; then set the same env vars. |

You do **not** need an extra “Elasticsearch inference API key” — the cluster **ELASTICSEARCH_API_KEY** is used for inference as well.
